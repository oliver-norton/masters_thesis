{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About \n",
    "\n",
    "The following is a barebones script of how one would export posts from Twitter using an API. Sensitive code has been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/virtualenv_python3/lib/python3.5/site-packages/OpenSSL/crypto.py:14: CryptographyDeprecationWarning: Python 3.5 support will be dropped in the next release of cryptography. Please upgrade your Python.\n",
      "  from cryptography import utils, x509\n",
      "/home/jupyter/virtualenv_python3/lib/python3.5/site-packages/arrow/arrow.py:28: DeprecationWarning: Arrow will drop support for Python 2.7 and 3.5 in the upcoming v1.0.0 release. Please upgrade to Python 3.6+ to continue receiving updates for Arrow.\n",
      "  DeprecationWarning,\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "########################\n",
    "\n",
    "import os\n",
    "from datetime import datetime, date, time, timedelta\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "#######\n",
    "###############\n",
    "\n",
    "import copy\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "##################\n",
    "import time \n",
    "import csv#\n",
    "from datetime import datetime, timedelta\n",
    "import arrow\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### QUERY #################################################################\n",
    "\n",
    "# we do it by week, until the end of our dates - but lets try one week at a time. \n",
    "\n",
    "parameters = #########################################\n",
    "\n",
    "big_start_date = datetime(2022, 9, 12, 0, 0, 0) \n",
    "big_end_date = datetime(2022, 10, 23, 23, 59, 59)\n",
    "\n",
    "filters = {##########################\n",
    "#     LIST OF KEYWORDS\n",
    "}\n",
    "\n",
    "# loop through each month within the specified time period, and collect data, store in pkl files\n",
    "for date_range in arrow.Arrow.span_range('week', big_start_date, big_end_date):\n",
    "    # extract start and end date for the current month\n",
    "    week_start = date_range[0].datetime\n",
    "    week_end = date_range[1].datetime - timedelta(seconds=1)\n",
    "\n",
    "    # create the filename for the current month's output\n",
    "    now = datetime.now()\n",
    "    file_name =  \"./output/twitter_posts_{}_{}_W{}_{}.pkl\".format(\n",
    "        week_start.strftime(\"%Y\"),\n",
    "        week_start.strftime(\"%b\").upper(),\n",
    "        week_start.strftime(\"%U\"),\n",
    "        now.strftime(\"%Y%m%d\"))\n",
    "\n",
    "    # set up the date filter for the current month\n",
    "    start_date= week_start \n",
    "    end_date= week_end\n",
    "\n",
    "    # make the API call with the current month's date filter\n",
    "    month_posts = search_get_all_posts(########)\n",
    "\n",
    "    # save the output to the current month's file\n",
    "    month_posts.to_pickle(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # for our one day time period, we should collect 20118 posts \n",
    "\n",
    "# but since we have a month function, it will collect for all of april, from 1st april to 30th of april (even though posts don't expist yet)\n",
    "\n",
    "# so this will be 89646 posts \n",
    "# 9 batches (9750*9)\n",
    "\n",
    "# see by batches how long is left \n",
    "\n",
    "# has taken 5 minutes maybe?\n",
    "\n",
    "# given how long it is taking, and how many posts there are - i think we should do by \n",
    "# 7 batches = 70000 posts \n",
    "\n",
    "#\n",
    "\n",
    "# always check how long it will take before doing anything like this! \n",
    "\n",
    "# there are 5245021 posts in reddit from 1st sept 2022 to 31st mar 2023\n",
    "\n",
    "# 86000 took 12 minutes\n",
    "\n",
    "# 5.245m will take 12*61 = 732 minutes \n",
    "# 12.2 hours \n",
    "# round up to 13 hours\n",
    "\n",
    "# so there are: 30 weeks to collect on\n",
    "# 7 months of data\n",
    "# one hour at a time\n",
    "\n",
    "# 31/13 = 2.38 weeks\n",
    "# so we do 2 iso week standards at a time.\n",
    "# since we have our image, we can use these nicely \n",
    "\n",
    "\n",
    "### ISO WEEKS\n",
    "\n",
    "2022:\n",
    "    W35-52\n",
    "    \n",
    "2023: \n",
    "    W1-13\n",
    "\n",
    "\n",
    " -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
